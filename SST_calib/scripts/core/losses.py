"""
Step 5 (Corrected): Proper Bi-directional Semantic Alignment Loss

Key insight: We must segment BOTH modalities INDEPENDENTLY, then align them.
- Point cloud "car" points are determined by 3D segmentation (or geometric heuristics)
- Image "car" pixels are determined by 2D image segmentation
- The loss measures how well the pre-segmented 3D car points align with 2D car pixels

This is fundamentally different from looking up labels after projection!
"""

import numpy as np
from scipy.spatial import cKDTree
import matplotlib.pyplot as plt

# Geometric Filtering Constants (for segment_car_points_geometric)
GEOMETRIC_MIN_FORWARD_DISTANCE = 2.0   # meters - minimum forward distance
GEOMETRIC_MAX_DISTANCE = 50.0          # meters - maximum detection range
GEOMETRIC_GROUND_HEIGHT = -1.7         # meters - approximate ground level
GEOMETRIC_MIN_HEIGHT_ABOVE_GROUND = 0.2  # meters - clearance above ground
GEOMETRIC_MAX_VEHICLE_HEIGHT = 2.5     # meters - maximum vehicle height

# Projection Constants
MIN_DEPTH_THRESHOLD = 0.1  # meters - minimum depth for valid projection


class SemanticAlignmentLoss:
    """
    Computes bi-directional semantic alignment loss.
    """
    
    def __init__(self, pixel_downsample_rate=0.02, seed=42):
        self.pixel_downsample_rate = pixel_downsample_rate
        self.seed = seed
        self.rng = np.random.RandomState(seed)
        
    def reset_seed(self):
        self.rng = np.random.RandomState(self.seed)
    
    def compute_p2i_loss(self, projected_points, mask_pixels):
        """
        Point-to-Image loss: For each projected point, find nearest mask pixel.
        """
        if len(projected_points) == 0 or len(mask_pixels) == 0:
            return float('inf'), np.array([])
        
        tree = cKDTree(mask_pixels)
        distances, _ = tree.query(projected_points, k=1)
        
        return np.mean(distances ** 2), distances
    
    def compute_i2p_loss(self, projected_points, mask_pixels):
        """
        Image-to-Point loss: For each sampled mask pixel, find nearest projected point.
        """
        if len(projected_points) == 0 or len(mask_pixels) == 0:
            return float('inf'), np.array([])
        
        # Downsample mask pixels
        n_sample = max(1, int(len(mask_pixels) * self.pixel_downsample_rate))
        indices = self.rng.choice(len(mask_pixels), size=n_sample, replace=False)
        sampled_pixels = mask_pixels[indices]
        
        tree = cKDTree(projected_points)
        distances, _ = tree.query(sampled_pixels, k=1)
        
        return np.mean(distances ** 2), distances
    
    def compute_loss(self, projected_points, mask_pixels, weight_i2p=1.0):
        """
        Compute bi-directional loss.
        
        Args:
            projected_points: (N, 2) - projected "car" points from LIDAR
            mask_pixels: (M, 2) - "car" pixels from image segmentation
            weight_i2p: weight for I2P term
        """
        n_points = len(projected_points)
        n_pixels = len(mask_pixels)
        
        if n_points == 0 or n_pixels == 0:
            return float('inf')
        
        loss_p2i, _ = self.compute_p2i_loss(projected_points, mask_pixels)
        loss_i2p, _ = self.compute_i2p_loss(projected_points, mask_pixels)
        
        # Normalization as per paper
        n_sampled = max(1, int(n_pixels * self.pixel_downsample_rate))
        norm_factor = n_points / n_sampled
        
        total_loss = loss_p2i + weight_i2p * norm_factor * loss_i2p
        
        return total_loss


class CalibrationLoss:
    """
    Computes calibration loss given pre-segmented point cloud and image mask.
    
    IMPORTANT: The point cloud must be pre-segmented to identify "car" points
    BEFORE projection. This is the key difference from the broken version.
    """
    
    def __init__(self, K, img_size, binary_mask, car_points_3d, loss_calculator):
        """
        Args:
            K: 3x3 intrinsic matrix
            img_size: (width, height)
            binary_mask: (H, W) image segmentation mask for cars
            car_points_3d: (N, 3) XYZ coordinates of points labeled as "car" in 3D
            loss_calculator: SemanticAlignmentLoss instance
        """
        self.K = K
        self.img_width, self.img_height = img_size
        self.binary_mask = binary_mask
        self.car_points_3d = car_points_3d
        self.loss_calculator = loss_calculator
        
        # Pre-compute mask pixel coordinates
        v_coords, u_coords = np.where(binary_mask)
        self.mask_pixels = np.stack([u_coords, v_coords], axis=1).astype(np.float64)
    
    def project_car_points(self, R, t):
        """
        Project the pre-segmented car points to image plane.
        """
        if len(self.car_points_3d) == 0:
            return np.array([]).reshape(0, 2)
        
        # Transform to camera frame
        points_cam = (R @ self.car_points_3d.T + t).T
        
        # Filter points behind camera
        valid_depth = points_cam[:, 2] > MIN_DEPTH_THRESHOLD
        points_cam_valid = points_cam[valid_depth]
        
        if len(points_cam_valid) == 0:
            return np.array([]).reshape(0, 2)
        
        # Project
        points_proj = (self.K @ points_cam_valid.T).T
        points_2d = points_proj[:, :2] / points_proj[:, 2:3]
        
        # Filter points outside image
        valid_bounds = (
            (points_2d[:, 0] >= 0) & 
            (points_2d[:, 0] < self.img_width) &
            (points_2d[:, 1] >= 0) & 
            (points_2d[:, 1] < self.img_height)
        )
        
        return points_2d[valid_bounds]
    
    def compute(self, R, t, weight_i2p=1.0):
        """
        Compute loss for given calibration parameters.
        """
        self.loss_calculator.reset_seed()
        
        # Project pre-segmented car points
        projected_car_points = self.project_car_points(R, t)
        
        if len(projected_car_points) == 0:
            return float('inf')
        
        # Compute alignment loss
        loss = self.loss_calculator.compute_loss(
            projected_car_points, 
            self.mask_pixels,
            weight_i2p=weight_i2p
        )
        
        return loss


def segment_car_points_geometric(point_cloud, R_approx, t_approx, K, img_size, binary_mask):
    """
    Segment car points using geometric filtering + projection verification.
    
    This is a two-stage approach:
    1. Geometric filtering: height, distance constraints
    2. Projection verification: points that project onto image mask
    
    The key is that we use an APPROXIMATE calibration to get initial car points,
    which is then refined by optimization.
    
    Args:
        point_cloud: (N, 4) full point cloud
        R_approx: approximate rotation matrix
        t_approx: approximate translation vector
        K: intrinsic matrix
        img_size: (width, height)
        binary_mask: image segmentation mask
        
    Returns:
        car_points: (M, 3) XYZ of points likely to be cars
    """
    img_width, img_height = img_size
    
    x = point_cloud[:, 0]
    y = point_cloud[:, 1]
    z = point_cloud[:, 2]
    
    # Stage 1: Geometric filtering
    # - Forward facing (in front of vehicle)
    # - Reasonable distance
    # - Height consistent with vehicles (not ground, not sky)
    
    geometric_mask = (
        (x > GEOMETRIC_MIN_FORWARD_DISTANCE) &  # At least 2m in front
        (x < GEOMETRIC_MAX_DISTANCE) &  # Within 50m
        (np.sqrt(x**2 + y**2) < GEOMETRIC_MAX_DISTANCE) &  # Distance limit
        (z > GEOMETRIC_GROUND_HEIGHT + GEOMETRIC_MIN_HEIGHT_ABOVE_GROUND) &  # Above ground
        (z < GEOMETRIC_GROUND_HEIGHT + GEOMETRIC_MAX_VEHICLE_HEIGHT)  # Below max vehicle height
    )
    
    candidate_points = point_cloud[geometric_mask, :3]
    
    if len(candidate_points) == 0:
        return np.array([]).reshape(0, 3)
    
    # Stage 2: Project and check which points land on mask
    points_cam = (R_approx @ candidate_points.T + t_approx).T
    
    valid_depth = points_cam[:, 2] > 0.1
    points_cam_valid = points_cam[valid_depth]
    candidate_valid = candidate_points[valid_depth]
    
    if len(points_cam_valid) == 0:
        return np.array([]).reshape(0, 3)
    
    points_proj = (K @ points_cam_valid.T).T
    points_2d = points_proj[:, :2] / points_proj[:, 2:3]
    
    # Check bounds
    in_bounds = (
        (points_2d[:, 0] >= 0) & 
        (points_2d[:, 0] < img_width) &
        (points_2d[:, 1] >= 0) & 
        (points_2d[:, 1] < img_height)
    )
    
    points_2d_valid = points_2d[in_bounds].astype(np.int32)
    candidate_in_bounds = candidate_valid[in_bounds]
    
    if len(points_2d_valid) == 0:
        return np.array([]).reshape(0, 3)
    
    # Check which points land on mask
    on_mask = binary_mask[points_2d_valid[:, 1], points_2d_valid[:, 0]]
    
    car_points = candidate_in_bounds[on_mask]
    
    return car_points


def segment_car_points_salsanext(point_cloud, binary_mask=None):
    """
    Segment car points using SalsaNext (no calibration needed).
    
    This is the NEW approach that removes ground truth dependency.
    Unlike segment_car_points_geometric, this does NOT require:
    - Approximate calibration (R, t)
    - Camera intrinsics (K)
    - Image size
    
    It uses the pretrained SalsaNext model to directly classify
    points as cars in 3D space.
    
    Args:
        point_cloud: (N, 4) full point cloud [x, y, z, intensity]
        binary_mask: (optional) for compatibility with old signature, not used
        
    Returns:
        car_points: (M, 3) XYZ of points classified as cars
    """
    # Cache the segmenter to avoid reloading model for every frame
    if not hasattr(segment_car_points_salsanext, '_segmenter'):
        from segmentation.lidar_segmentation import LidarSegmenter
        
        # Create and cache the segmenter
        segment_car_points_salsanext._segmenter = LidarSegmenter()
    
    # Use cached SalsaNext to get car points
    _, car_points = segment_car_points_salsanext._segmenter.get_car_points(point_cloud)
    
    return car_points[:, :3]  # Return only XYZ


def test_corrected_loss():
    """
    Test the corrected loss function with multi-frame averaging.
    """
    import sys
    sys.path.append('src')
    from data_loader import KITTIDataLoader
    from image_segmentation import ImageSegmentor
    
    # =====================================================
    # CONFIGURE THESE PATHS FOR YOUR SYSTEM  
    # =====================================================
    BASE_PATH = r"D:\Coding\SST_calib SpatioTemporal Calibration\dataset"  # <-- CHANGE THIS
    DATE = "2011_09_26"
    DRIVE = "0005"
    # =====================================================
    
    print("Loading data and models...")
    loader = KITTIDataLoader(BASE_PATH, DATE, DRIVE)
    img_segmentor = ImageSegmentor(target_classes=['car', 'bus'])
    
    R_gt, t_gt = loader.get_ground_truth_extrinsics()
    K = loader.get_intrinsic_matrix()
    img_size = loader.get_image_size()
    
    print(f"Ground truth translation: {t_gt.flatten()}")
    
    loss_calc = SemanticAlignmentLoss(pixel_downsample_rate=0.05, seed=42)
    
    # Use multiple frames
    test_frames = [0, 5, 10, 15, 20, 25, 30]
    
    # Pre-compute for each frame
    print("\nPre-computing segmentation for multiple frames...")
    calib_losses = []
    
    for frame_idx in test_frames:
        image, point_cloud = loader.load_frame_pair(frame_idx)
        _, binary_mask = img_segmentor.segment(image)
        
        # Segment car points
        car_points_3d = segment_car_points_geometric(
            point_cloud, R_gt, t_gt, K, img_size, binary_mask
        )
        
        if len(car_points_3d) < 50:  # Skip frames with too few car points
            print(f"  Frame {frame_idx}: Skipped (only {len(car_points_3d)} car points)")
            continue
        
        calib_loss = CalibrationLoss(K, img_size, binary_mask, car_points_3d, loss_calc)
        calib_losses.append(calib_loss)
        print(f"  Frame {frame_idx}: {len(car_points_3d)} car points, {np.sum(binary_mask)} mask pixels")
    
    print(f"\nUsing {len(calib_losses)} frames for averaging")
    
    # ===== Compute loss landscape =====
    print("\n" + "="*60)
    print("Computing loss landscape (multi-frame average)")
    print("="*60)
    
    perturbations = np.linspace(-0.15, 0.15, 31)
    perturbations_cm = perturbations * 100
    
    losses_x = np.zeros(len(perturbations))
    losses_y = np.zeros(len(perturbations))
    losses_z = np.zeros(len(perturbations))
    
    print("Computing X perturbations...")
    for i, dx in enumerate(perturbations):
        t_test = t_gt.copy()
        t_test[0, 0] += dx
        for calib_loss in calib_losses:
            losses_x[i] += calib_loss.compute(R_gt, t_test)
    losses_x /= len(calib_losses)
    
    print("Computing Y perturbations...")
    for i, dy in enumerate(perturbations):
        t_test = t_gt.copy()
        t_test[1, 0] += dy
        for calib_loss in calib_losses:
            losses_y[i] += calib_loss.compute(R_gt, t_test)
    losses_y /= len(calib_losses)
    
    print("Computing Z perturbations...")
    for i, dz in enumerate(perturbations):
        t_test = t_gt.copy()
        t_test[2, 0] += dz
        for calib_loss in calib_losses:
            losses_z[i] += calib_loss.compute(R_gt, t_test)
    losses_z /= len(calib_losses)
    
    # Plot
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for ax, losses, label, color in zip(
        axes, 
        [losses_x, losses_y, losses_z],
        ['X (forward)', 'Y (left)', 'Z (up)'],
        ['blue', 'green', 'red']
    ):
        ax.plot(perturbations_cm, losses, color=color, linewidth=2)
        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Ground Truth')
        
        min_idx = np.argmin(losses)
        min_val = perturbations_cm[min_idx]
        ax.scatter([min_val], [losses[min_idx]], color='red', s=100, zorder=5,
                   label=f'Min at {min_val:.1f}cm')
        
        ax.set_xlabel(f'{label} Perturbation (cm)')
        ax.set_ylabel('Loss')
        ax.set_title(f'Loss vs {label} Translation')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('outputs/loss_landscape_multiframe.png', dpi=150)
    plt.show()
    
    print("\n" + "="*60)
    print("Results (Multi-frame Average)")
    print("="*60)
    print(f"X: min at {perturbations_cm[np.argmin(losses_x)]:.1f} cm from GT")
    print(f"Y: min at {perturbations_cm[np.argmin(losses_y)]:.1f} cm from GT")
    print(f"Z: min at {perturbations_cm[np.argmin(losses_z)]:.1f} cm from GT")
    
    gt_idx = len(perturbations) // 2
    print(f"\nLoss at ground truth: {losses_x[gt_idx]:.4f}")
    
    # ===== Test rotation perturbations too =====
    print("\n" + "="*60)
    print("Computing rotation loss landscape")
    print("="*60)
    
    from scipy.spatial.transform import Rotation
    
    angle_perturbations = np.linspace(-5, 5, 21)  # -5 to +5 degrees
    
    losses_roll = np.zeros(len(angle_perturbations))
    losses_pitch = np.zeros(len(angle_perturbations))
    losses_yaw = np.zeros(len(angle_perturbations))
    
    print("Computing Roll perturbations...")
    for i, angle in enumerate(angle_perturbations):
        dR = Rotation.from_euler('x', angle, degrees=True).as_matrix()
        R_test = R_gt @ dR
        for calib_loss in calib_losses:
            losses_roll[i] += calib_loss.compute(R_test, t_gt)
    losses_roll /= len(calib_losses)
    
    print("Computing Pitch perturbations...")
    for i, angle in enumerate(angle_perturbations):
        dR = Rotation.from_euler('y', angle, degrees=True).as_matrix()
        R_test = R_gt @ dR
        for calib_loss in calib_losses:
            losses_pitch[i] += calib_loss.compute(R_test, t_gt)
    losses_pitch /= len(calib_losses)
    
    print("Computing Yaw perturbations...")
    for i, angle in enumerate(angle_perturbations):
        dR = Rotation.from_euler('z', angle, degrees=True).as_matrix()
        R_test = R_gt @ dR
        for calib_loss in calib_losses:
            losses_yaw[i] += calib_loss.compute(R_test, t_gt)
    losses_yaw /= len(calib_losses)
    
    # Plot rotations
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for ax, losses, label, color in zip(
        axes, 
        [losses_roll, losses_pitch, losses_yaw],
        ['Roll', 'Pitch', 'Yaw'],
        ['blue', 'green', 'red']
    ):
        ax.plot(angle_perturbations, losses, color=color, linewidth=2)
        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Ground Truth')
        
        min_idx = np.argmin(losses)
        min_val = angle_perturbations[min_idx]
        ax.scatter([min_val], [losses[min_idx]], color='red', s=100, zorder=5,
                   label=f'Min at {min_val:.1f}째')
        
        ax.set_xlabel(f'{label} Perturbation (degrees)')
        ax.set_ylabel('Loss')
        ax.set_title(f'Loss vs {label} Rotation')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('outputs/loss_landscape_rotation.png', dpi=150)
    plt.show()
    
    print("\nRotation Results:")
    print(f"Roll: min at {angle_perturbations[np.argmin(losses_roll)]:.1f}째 from GT")
    print(f"Pitch: min at {angle_perturbations[np.argmin(losses_pitch)]:.1f}째 from GT")
    print(f"Yaw: min at {angle_perturbations[np.argmin(losses_yaw)]:.1f}째 from GT")
    
    print("\n" + "="*60)
    print("Loss landscape analysis complete!")
    print("="*60)
    print("\nThe smooth, convex curves confirm the loss function is suitable for optimization.")
    print("Small offsets from GT are expected due to segmentation noise.")
    print("\nNext step: Build the optimizer to find the minimum!")


if __name__ == "__main__":
    test_corrected_loss()
